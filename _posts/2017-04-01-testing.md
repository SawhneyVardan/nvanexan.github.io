---
layout: post
title: "Testing"
date: 2017-04-01
---

Well. Finally got around to putting this old website together. Neat thing about it - powered by [Jekyll](http://jekyllrb.com) and I can use Markdown to author my posts. It actually is a lot easier than I thought it was going to be.

# Meeting re Automated Testing  

##### March 28, 2017

---

## Executive Summary

I met with a couple members of your QA team and one of your developers. After your introduction[^1], we discussed the following topics:

- Appropriate testing frameworks for Angular 1 applications
- Effective strategies for code organization of automated test suites within an Angular application
- Effective strategies for dealing with test failures, including taking screenshots
- TRX reports and their utility within TFS / VSO
- How to achieve better console logging, and the typical setup / interaction between VSO and Browserstack for e2e tests
- Best practices for behaviour-driven development ("BDD")

**Summary of recommendations:**

- Use Protractor for testing (it wraps Selenium, which you are currently using) because it is Angular aware and will ease your coding burden
- Organize your testing code into generic helpers and page / view-specific helpers, which your tests can then leverage (i.e. DRY out your test code)
- Use the "multicapabilities" property in your protractor / browserstack config file to run tests against multiple browsers in one fell swoop
- Have protractor take screenshots when a test fails, so you can see what the browser was showing at time of failure
- Use a test framework like Jasmine (or C# equivalent) to generate a TRX report at the end of your testing, which can be read by TFS / VSO
- Use smoke tests to test all the routes of your application, then run regression tests against defined user behaviours - focus on writing small tests, which can be chained together, so that your error / failure logs are more meaningful for you

## Key Links

- Example source code: [https://github.com/nvanexan/ux-workflow-demo](https://github.com/nvanexan/ux-workflow-demo)
- Protractor: [http://www.protractortest.org/#/](http://www.protractortest.org/#/)
- Jasmine: [https://jasmine.github.io/](https://jasmine.github.io/)
- Configuring Protractor with browserstack: [https://www.browserstack.com/automate/protractor](https://www.browserstack.com/automate/protractor)

## Recommendations

### Webdriver: Use Protractor

Shane explained that he's coding against vanilla Selenium Webdriver without any additional helper libraries and without any testing framework in place for the front end of the application. Selenium provides an API for piloting browsers, but it is not aware of Angular. As a result, Shane has to hardcode wait commands for a number of steps in his tests, which is (1) tedious and (2) may be fragile when running tests from slower machines in the cloud through Browserstack (which may have longer wait times).

**Recommendations:**

- Use Protractor instead of coding against Selenium natively. See [http://www.protractortest.org/#/](http://www.protractortest.org/#/)
- Protractor is built on top of WebDriverJS, but it's built for Angular apps. This means it can use Angular-specific locator strategies (e.g. find by ngmodel or find by repeater). A full list of locator strategies is found in the docs with excellent documentation / code patterns. See [http://www.protractortest.org/#/api](http://www.protractortest.org/#/api)
- Protractor also supports "automatic waiting", which means Shane won't have to worry about waiting for his test and webpage to sync.
- Protractor can be installed easily on your testing machine via npm and can be run from the command line. Details on getting it installed are available here: [http://www.protractortest.org/#/](http://www.protractortest.org/#/)
- Example source code to run protractor locally, locally against browserstack, and remotely (in the cloud) against browserstack, can be found in the above mentioned repo at:
	- **local:** e2e-tests > local.conf.js
	- **local against browserstack:** e2e-tests > bstack.local.conf.js
	- **remotely against browserstack:** e2e-tests > bstack.remote.conf.js + bstack.module.conf.js

### Code Organization

Shane is currently testing his tests in a single file. This is great for testing a specific test locally, but over time will be difficult to maintain / scale.  

**Recommendations:**

- Organize your code so you can create an API for your tests, as follows...
- Set up an e2e test folder in your front-end source directory to house your testing code
- Create helpers - refactor your existing test code so that methods that are commonly repeated by users throughout your application (i.e. fill out text box, select from dropdown) are put into separate helper classes that can be used throughout your test suite
- Organize your code by page - refactor your existing test code again so methods that are specific to a specific page / view (i.e. shopping cart) are stored in a specific class for that page
- Separate your tests / specs into multiple files (e.g. "smoke tests", "regression test 1", "regression test 2") -> Shane and Jeff should work together on figuring out what the different isolated tests will be
- Write your tests / specs in a manner that utilizes the helper libraries you have organized

### Running Against Browserstack

Shane mentioned that he's interested in running against browserstack with multiple browser versions / OS configurations. This can be done with Protractor, as per below...

**Recommendations:**

- You can run against multiple kinds of browsers and OS combinations when using browserstack with protractor, using the "multicapabilities" property
- In your configuration file, you would specify the browsers and versions you want to use in your config file like so:

```
exports.config = {
  'seleniumAddress': 'http://hub-cloud.browserstack.com/wd/hub',

  'commonCapabilities': {
    'browserstack.user': ************,
    'browserstack.key': ************
  },

  'multiCapabilities': [{
    'browserName': 'Chrome'
  },{
    'browserName': 'Safari'
  },{
    'browserName': 'Firefox'
  },{
    'browserName': 'IE'
  }]
};

// Code to support common capabilities
exports.config.multiCapabilities.forEach(function(caps){
  for(var i in exports.config.commonCapabilities) caps[i] = caps[i] || exports.config.commonCapabilities[i];
});
```

- Full documentation on how you can configure Browserstack is here: [https://www.browserstack.com/automate/protractor](https://www.browserstack.com/automate/protractor) - see specifically the section called "Running Parallel Tests"

### Handling Test Failures

Shane mentioned you are looking to more easily identify failures when they arise.

**Recommendations:**

- Use Protractor to take a screenshot when a test fails, so you can review what was happening in the browser at the time the test failed. See the sample code repo in the e2e-tests > tests > screenshots.spec.js file for an example at [https://github.com/nvanexan/ux-workflow-demo/blob/master/e2e-tests/tests/screenshot.js](https://github.com/nvanexan/ux-workflow-demo/blob/master/e2e-tests/tests/screenshot.js)
- Use a testing framework like Jasmine, which will give you better console logs of your test and identify when and where a test has failed. See Jasmine: [https://jasmine.github.io/](https://jasmine.github.io/)
- Log what is happening in your tests to the console liberally throughout your tests with console.log - it will help you determine where in your test your code failed
- Write small tests, so you can isolate failures more easily
- Output your test results to a TRX report, which can be read in TFS / VSO. If you are using Jasmine as your testing framework, for example, you can do this with the jasmine-trx-reporter library from npm. See documentation to set it up here: [https://github.com/papa-pep/jasmine-trx-reporter](https://github.com/papa-pep/jasmine-trx-reporter)
- Run your tests against browserstack with 'browserstack.debug' set to true - this will create more verbose logs that you can then inspect in Browserstack after the test has run

```
exports.config = {
  'seleniumAddress': 'http://hub-cloud.browserstack.com/wd/hub',

  'capabilities': {
    'browserstack.user': ************,
    'browserstack.key': ************,
    'browserstack.debug': true
  }
};
```

### Test Reporting

Shane mentioned that at some point you'd like to see reports of the pass / failure of a test suite in your release process.

**Recommendations:**

- You should run automated e2e tests when you release to a test environment
- As I demonstrated to your team, if you are using VSO, and you are generating a TRX report at the end of your test process, you can publish that TRX report as an artifact, which will then be displayed in your "tests" section


### BDD Test Strategy

Shane mentioned that you are using actual user accounts for testing right now. This is great for now, but may cause a headache later when you want to look for a specific result (e.g. you know in the database that user should see X price for a product but different users may not have that product).

**Recommendations:**

- Set up a business acceptance test ("BAT") account in your application for testing purposes, where you can define and control the products, etc. that the user has available
- This will allow you to do consistent testing across environments and eases the test writing burden because your testing team knows everything about that user's configuration and can modify it if necessary
- You should also consider setting up a suite of smoke tests that run before any regression tests that go and hit all of the routes available to your test user in the application. This way you will at least know a user can access all of the pages available to it with every release.


[^1]: This is my first footnote
